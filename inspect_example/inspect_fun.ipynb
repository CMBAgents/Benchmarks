{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25795e67-7a56-4d01-84d0-375f2225a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import inspect\n",
    "import random\n",
    "import sys\n",
    "import asyncio\n",
    "import pandas as pd                    \n",
    "import inspect\n",
    "from inspect_ai import Task, task, eval as inspect_eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.scorer import exact   \n",
    "from inspect_ai.solver import solver, TaskState, Generate\n",
    "from inspect_ai.scorer import scorer, mean, stderr, CORRECT, INCORRECT, Score\n",
    "from inspect_ai.solver import TaskState\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c114b6e3-5cb7-47d1-9977-70d34afe3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "@solver\n",
    "def submission(source: str):\n",
    "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
    "        state.output.completion = source\n",
    "        return state\n",
    "    return solve\n",
    "\n",
    "\n",
    "@scorer(metrics=[mean()])\n",
    "def same_io_dynamic(num_tests: int = 50, low: int = -100, high: int = 100):\n",
    "    \"\"\"\n",
    "    Test functions of any arity by:\n",
    "      1. Parsing `reference_code` to get the correct function name.\n",
    "      2. Loading that same name from the candidate file.\n",
    "      3. Running randomized I/O tests with numpy→Python coercion.\n",
    "    \"\"\"\n",
    "    async def score(state: TaskState, target) -> Score:\n",
    "        cand_src = state.output.completion\n",
    "        ref_src  = getattr(target, \"text\", target)\n",
    "\n",
    "\n",
    "        ref_tree = ast.parse(ref_src)\n",
    "        ref_defs = [n for n in ref_tree.body if isinstance(n, ast.FunctionDef)]\n",
    "        if not ref_defs:\n",
    "            return Score(value=INCORRECT,\n",
    "                         answer=cand_src,\n",
    "                         explanation=\"Reference has no function definition.\")\n",
    "        fn_name = ref_defs[0].name\n",
    "\n",
    "\n",
    "        def load_named_fn(src: str, name: str):\n",
    "            ns = {}\n",
    "            exec(compile(src, \"<string>\", \"exec\"), ns)\n",
    "            if name not in ns or not callable(ns[name]):\n",
    "                raise ValueError(f\"Function {name!r} not found in candidate.\")\n",
    "            return ns[name]\n",
    "\n",
    "        try:\n",
    "            cand_fn = load_named_fn(cand_src, fn_name)\n",
    "            ref_fn  = load_named_fn(ref_src,  fn_name)\n",
    "        except Exception as e:\n",
    "            return Score(value=INCORRECT,\n",
    "                         answer=cand_src,\n",
    "                         explanation=f\"Load error: {e}\")\n",
    "\n",
    "\n",
    "        sig = inspect.signature(ref_fn)\n",
    "        param_count = len(sig.parameters)\n",
    "\n",
    "        for _ in range(num_tests):\n",
    "            args = [random.randint(low, high) for _ in range(param_count)]\n",
    "            try:\n",
    "                c_out = cand_fn(*args)\n",
    "                r_out = ref_fn(*args)\n",
    "            except Exception as e:\n",
    "                return Score(value=INCORRECT,\n",
    "                             answer=cand_src,\n",
    "                             explanation=f\"Runtime error on {args}: {e}\")\n",
    "\n",
    "            c_val = c_out.item() if hasattr(c_out, \"item\") else c_out\n",
    "            r_val = r_out.item() if hasattr(r_out, \"item\") else r_out\n",
    "\n",
    "            if c_val != r_val:\n",
    "                return Score(\n",
    "                    value=INCORRECT,\n",
    "                    answer=cand_src,\n",
    "                    explanation=(\n",
    "                        f\"Mismatch for inputs {args}: got {c_val} \"\n",
    "                        f\"({type(c_val)}), expected {r_val} ({type(r_val)})\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return Score(value=CORRECT, answer=cand_src)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "@scorer(metrics=[mean()])\n",
    "def content_similarity():\n",
    "    \"\"\"\n",
    "    Compute content similarity between the candidate and reference functions\n",
    "    by comparing their AST dumps via difflib.\n",
    "    Returns a ratio in [0,1] indicating structural similarity.\n",
    "    \"\"\"\n",
    "    async def score(state: TaskState, target) -> Score:\n",
    "\n",
    "        cand_src = state.output.completion\n",
    "        ref_src = getattr(target, \"text\", target)\n",
    "\n",
    "\n",
    "        try:\n",
    "            cand_ast = ast.parse(cand_src)\n",
    "            ref_ast = ast.parse(ref_src)\n",
    "        except SyntaxError as e:\n",
    "            return Score(value=0.0, answer=cand_src, explanation=f\"Syntax error during parse: {e}\")\n",
    "\n",
    "        cand_dump = ast.dump(cand_ast, annotate_fields=False)\n",
    "        ref_dump = ast.dump(ref_ast, annotate_fields=False)\n",
    "\n",
    "\n",
    "        ratio = difflib.SequenceMatcher(None, cand_dump, ref_dump).ratio()\n",
    "\n",
    "\n",
    "        return Score(value=ratio, answer=cand_src, explanation=f\"AST similarity ratio: {ratio:.2f}\")\n",
    "\n",
    "    return score\n",
    "\n",
    "@task\n",
    "def code_match_dynamic(reference,candidate):\n",
    "    return Task(\n",
    "        dataset=[Sample(\n",
    "            input=\"Return Python code solving the task.\",\n",
    "            target=reference\n",
    "        )],\n",
    "        solver=[submission(candidate)],\n",
    "        scorer=same_io_dynamic()\n",
    "    )\n",
    "@task\n",
    "def code_match_with_similarity(reference,candidate): \n",
    "    return Task(\n",
    "        dataset=[Sample(input=\"Return Python code solving the task.\", target=reference)],\n",
    "        solver=[submission(candidate)],  \n",
    "        scorer=[same_io_dynamic(),content_similarity()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d055c2-66e5-417e-9549-1d1d814d3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_PATH = \"filename.py\"\n",
    "df = pd.DataFrame({\n",
    "    'reference_code': ['def add_numbers(a, b, c):\\n    return a + b + c']\n",
    "})\n",
    "reference_code = df.reference_code.iloc[0]\n",
    "with open(CANDIDATE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    candidate_code = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9cc54a4-db14-4a88-a393-9b5a8ad11377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">code_match_with_similarity (1 sample): openai/gpt-4.1-2025-04-14</span> ──────────────────────────────────────────────╮\n",
       "│ reference: def add_numbers(a, b, c):                                               <span style=\"color: #000080; text-decoration-color: #000080\">          dataset: (samples)</span> │\n",
       "│     return a + b + c, candidate: import numpy as np                                                             │\n",
       "│ def add_numbers(a, b, c):                                                                                       │\n",
       "│     return np.sum([a,b,c])                                                                                      │\n",
       "│                                                                                                                 │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">total time:                                                   </span><span style=\"color: #808080; text-decoration-color: #808080\">  0:00:00                                        </span> │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">same_io_dynamic/mean: 1.0</span><span style=\"color: #008000; text-decoration-color: #008000\">  </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">content_similarity/mean[mean]: 0.639</span>                                                 │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">Log:</span> <a href=\"logs/2025-05-16T08-15-43-07-00_code-match-with-similarity_K9nMDUvQvEeTXxvCC8MYcj.eval\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080\">logs/2025-05-16T08-15-43-07-00_code-match-with-similarity_K9nMDUvQvEeTXxvCC8MYcj.eval</span></a>                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ \u001b[1mcode_match_with_similarity (1 sample): openai/gpt-4.1-2025-04-14\u001b[0m ──────────────────────────────────────────────╮\n",
       "│ reference: def add_numbers(a, b, c):                                                         dataset: (samples) │\n",
       "│     return a + b + c, candidate: import numpy as np                                                             │\n",
       "│ def add_numbers(a, b, c):                                                                                       │\n",
       "│     return np.sum([a,b,c])                                                                                      │\n",
       "│                                                                                                                 │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[1mtotal time:                                                   \u001b[0m  0:00:00                                         │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[1msame_io_dynamic/mean: 1.0\u001b[0m  \u001b[1mcontent_similarity/mean[mean]: 0.639\u001b[0m                                                 │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[1mLog:\u001b[0m \u001b]8;id=785620;logs/2025-05-16T08-15-43-07-00_code-match-with-similarity_K9nMDUvQvEeTXxvCC8MYcj.eval\u001b\\logs/2025-05-16T08-15-43-07-00_code-match-with-similarity_K9nMDUvQvEeTXxvCC8MYcj.eval\u001b]8;;\u001b\\                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logs = inspect_eval(code_match_with_similarity(reference_code,candidate_code), model=\"openai/gpt-4.1-2025-04-14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42f5a528-a21c-4a34-9a4c-532aef4c3a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l ={}\n",
    "for i in range(2):\n",
    "    l[logs[0].results.scores[i].name] = logs[0].results.scores[i].metrics['mean'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da0dbed6-14eb-4394-aa41-e15ae5c5e2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'same_io_dynamic': 1.0, 'content_similarity': 0.6385964912280702}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b62733-0609-404d-9671-42bb98461c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cmbagent_env)",
   "language": "python",
   "name": "cmbagent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
