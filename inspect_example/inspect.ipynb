{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7011d6b6-13ff-485b-b577-09b2c69e6af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">code_match_dynamic (1 sample): openai/gpt-4.1-2025-04-14</span> ──────────────────────────────────────────────────────╮\n",
       "│       <span style=\"color: #000080; text-decoration-color: #000080\">                                                                                       dataset: (samples)</span> │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">total time:                                                   </span><span style=\"color: #808080; text-decoration-color: #808080\">  0:00:00                                        </span> │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">mean: 1.0</span>                                                                                                       │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">Log:</span> <a href=\"logs/2025-05-15T21-38-33-07-00_code-match-dynamic_BLesYgsvSNoNhArXt3wBWP.eval\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080\">logs/2025-05-15T21-38-33-07-00_code-match-dynamic_BLesYgsvSNoNhArXt3wBWP.eval</span></a>                              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ \u001b[1mcode_match_dynamic (1 sample): openai/gpt-4.1-2025-04-14\u001b[0m ──────────────────────────────────────────────────────╮\n",
       "│                                                                                              dataset: (samples) │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[1mtotal time:                                                   \u001b[0m  0:00:00                                         │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[1mmean: 1.0\u001b[0m                                                                                                       │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[1mLog:\u001b[0m \u001b]8;id=714825;logs/2025-05-15T21-38-33-07-00_code-match-dynamic_BLesYgsvSNoNhArXt3wBWP.eval\u001b\\logs/2025-05-15T21-38-33-07-00_code-match-dynamic_BLesYgsvSNoNhArXt3wBWP.eval\u001b]8;;\u001b\\                              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ast\n",
    "import inspect\n",
    "import random\n",
    "import sys\n",
    "import asyncio\n",
    "import pandas as pd                    \n",
    "import inspect\n",
    "from inspect_ai import Task, task, eval as inspect_eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.scorer import exact   \n",
    "from inspect_ai.solver import solver, TaskState, Generate\n",
    "from inspect_ai.scorer import scorer, mean, stderr, CORRECT, INCORRECT, Score\n",
    "from inspect_ai.solver import TaskState\n",
    "import difflib\n",
    "\n",
    "\n",
    "CANDIDATE_PATH = \"filename.py\"\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'reference_code': ['def add_numbers(a, b, c):\\n    return a + b + c']\n",
    "})\n",
    "reference_code = df.reference_code.iloc[0]\n",
    "\n",
    "with open(CANDIDATE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    candidate_code = f.read()\n",
    "\n",
    "@solver\n",
    "def submission(source: str = candidate_code):\n",
    "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
    "        state.output.completion = source\n",
    "        return state\n",
    "    return solve\n",
    "\n",
    "\n",
    "@scorer(metrics=[mean()])\n",
    "def same_io_dynamic(num_tests: int = 50, low: int = -100, high: int = 100):\n",
    "    \"\"\"\n",
    "    Test functions of any arity by:\n",
    "      1. Parsing `reference_code` to get the correct function name.\n",
    "      2. Loading that same name from the candidate file.\n",
    "      3. Running randomized I/O tests with numpy→Python coercion.\n",
    "    \"\"\"\n",
    "    async def score(state: TaskState, target) -> Score:\n",
    "        cand_src = state.output.completion\n",
    "        ref_src  = getattr(target, \"text\", target)\n",
    "\n",
    "\n",
    "        ref_tree = ast.parse(ref_src)\n",
    "        ref_defs = [n for n in ref_tree.body if isinstance(n, ast.FunctionDef)]\n",
    "        if not ref_defs:\n",
    "            return Score(value=INCORRECT,\n",
    "                         answer=cand_src,\n",
    "                         explanation=\"Reference has no function definition.\")\n",
    "        fn_name = ref_defs[0].name\n",
    "\n",
    "\n",
    "        def load_named_fn(src: str, name: str):\n",
    "            ns = {}\n",
    "            exec(compile(src, \"<string>\", \"exec\"), ns)\n",
    "            if name not in ns or not callable(ns[name]):\n",
    "                raise ValueError(f\"Function {name!r} not found in candidate.\")\n",
    "            return ns[name]\n",
    "\n",
    "        try:\n",
    "            cand_fn = load_named_fn(cand_src, fn_name)\n",
    "            ref_fn  = load_named_fn(ref_src,  fn_name)\n",
    "        except Exception as e:\n",
    "            return Score(value=INCORRECT,\n",
    "                         answer=cand_src,\n",
    "                         explanation=f\"Load error: {e}\")\n",
    "\n",
    "\n",
    "        sig = inspect.signature(ref_fn)\n",
    "        param_count = len(sig.parameters)\n",
    "\n",
    "        for _ in range(num_tests):\n",
    "            args = [random.randint(low, high) for _ in range(param_count)]\n",
    "            try:\n",
    "                c_out = cand_fn(*args)\n",
    "                r_out = ref_fn(*args)\n",
    "            except Exception as e:\n",
    "                return Score(value=INCORRECT,\n",
    "                             answer=cand_src,\n",
    "                             explanation=f\"Runtime error on {args}: {e}\")\n",
    "\n",
    "            c_val = c_out.item() if hasattr(c_out, \"item\") else c_out\n",
    "            r_val = r_out.item() if hasattr(r_out, \"item\") else r_out\n",
    "\n",
    "            if c_val != r_val:\n",
    "                return Score(\n",
    "                    value=INCORRECT,\n",
    "                    answer=cand_src,\n",
    "                    explanation=(\n",
    "                        f\"Mismatch for inputs {args}: got {c_val} \"\n",
    "                        f\"({type(c_val)}), expected {r_val} ({type(r_val)})\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return Score(value=CORRECT, answer=cand_src)\n",
    "\n",
    "    return score\n",
    "\n",
    "@task\n",
    "def code_match_dynamic():\n",
    "    return Task(\n",
    "        dataset=[Sample(\n",
    "            input=\"Return Python code solving the task.\",\n",
    "            target=reference_code\n",
    "        )],\n",
    "        solver=[submission()],\n",
    "        scorer=same_io_dynamic()\n",
    "    )\n",
    "logs = inspect_eval(code_match_dynamic(), model=\"openai/gpt-4.1-2025-04-14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "294e59be-368c-40a4-989b-84e1a29a284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@scorer(metrics=[mean()])\n",
    "def content_similarity():\n",
    "    \"\"\"\n",
    "    Compute content similarity between the candidate and reference functions\n",
    "    by comparing their AST dumps via difflib.\n",
    "    Returns a ratio in [0,1] indicating structural similarity.\n",
    "    \"\"\"\n",
    "    async def score(state: TaskState, target) -> Score:\n",
    "\n",
    "        cand_src = state.output.completion\n",
    "        ref_src = getattr(target, \"text\", target)\n",
    "\n",
    "\n",
    "        try:\n",
    "            cand_ast = ast.parse(cand_src)\n",
    "            ref_ast = ast.parse(ref_src)\n",
    "        except SyntaxError as e:\n",
    "            return Score(value=0.0, answer=cand_src, explanation=f\"Syntax error during parse: {e}\")\n",
    "\n",
    "        cand_dump = ast.dump(cand_ast, annotate_fields=False)\n",
    "        ref_dump = ast.dump(ref_ast, annotate_fields=False)\n",
    "\n",
    "\n",
    "        ratio = difflib.SequenceMatcher(None, cand_dump, ref_dump).ratio()\n",
    "\n",
    "\n",
    "        return Score(value=ratio, answer=cand_src, explanation=f\"AST similarity ratio: {ratio:.2f}\")\n",
    "\n",
    "    return score\n",
    "\n",
    "from inspect_ai import Task, task\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import solver, TaskState, Generate\n",
    "\n",
    "@task\n",
    "def code_match_with_similarity():\n",
    "    reference = reference_code  \n",
    "    return Task(\n",
    "        dataset=[Sample(input=\"Return Python code solving the task.\", target=reference)],\n",
    "        solver=[submission()],  \n",
    "        scorer=content_similarity()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6108ecac-1bcb-4534-9687-4cc8ab5bd965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">code_match_with_similarity (1 sample): openai/gpt-4.1-2025-04-14</span> ──────────────────────────────────────────────╮\n",
       "│       <span style=\"color: #000080; text-decoration-color: #000080\">                                                                                       dataset: (samples)</span> │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">total time:                                                   </span><span style=\"color: #808080; text-decoration-color: #808080\">  0:00:00                                        </span> │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">mean: 0.639</span>                                                                                                     │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">Log:</span> <a href=\"logs/2025-05-15T21-38-37-07-00_code-match-with-similarity_J68cMTLcseKcttsnGih9cN.eval\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080\">logs/2025-05-15T21-38-37-07-00_code-match-with-similarity_J68cMTLcseKcttsnGih9cN.eval</span></a>                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ \u001b[1mcode_match_with_similarity (1 sample): openai/gpt-4.1-2025-04-14\u001b[0m ──────────────────────────────────────────────╮\n",
       "│                                                                                              dataset: (samples) │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[1mtotal time:                                                   \u001b[0m  0:00:00                                         │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[1mmean: 0.639\u001b[0m                                                                                                     │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[1mLog:\u001b[0m \u001b]8;id=311342;logs/2025-05-15T21-38-37-07-00_code-match-with-similarity_J68cMTLcseKcttsnGih9cN.eval\u001b\\logs/2025-05-15T21-38-37-07-00_code-match-with-similarity_J68cMTLcseKcttsnGih9cN.eval\u001b]8;;\u001b\\                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logs = inspect_eval(\n",
    "    code_match_with_similarity(),\n",
    "    model=\"openai/gpt-4.1-2025-04-14\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ba8f8-8a6c-464b-b37b-033623312b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cmbagent_env)",
   "language": "python",
   "name": "cmbagent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
